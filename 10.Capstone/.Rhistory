head(tail(a,10000),10)
head(tail(a,100000),10)
head(tail(a,10000000),10)
rm(list-ls())
rm(list=ls())
twitter<-readLines("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt")
corpus<-Corpus(VectorSource(twitter))
corpus<-tm_map(corpus, content_transformer(tolower))
final<-tm_map(final, removePunctuation)
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeNumbers)
?removePunctuation()
corpus<-term_stats(corpus,ngrams = 2:3)
head(corpus)
order(corpus)
class(corpus)
max(corpus$count)
head(corpus, 100)
corpus3<-term_stats(corpus,ngrams = 3)
rm(list=ls())
data("mtcars")
head(mtcars)
mtcars
library(caret)
inTrain<-createDataPartition(y=mtcars$mpg, p=0.75, list=0)
Train<-train[inTrain,]
Training<-mtcars[inTrain,]
Testing<-mtcars[-inTrain,]
tcontrol=trainControl(method = "cv", number=5)
fit_RF<-train(mpg ~ ., data=Training, method = "rf", trControl=trcontrol)
fit_RF<-train(mpg ~ ., data=Training, method = "rf", trControl=tcontrol)
pred_RF<-predict(fit_RF, newdata = Testing)
confusionMatrix(Testing$mpg,pred_RF)
confusionMatrix(Testing$mpg,pred_RF)
Testing$%mpg
Testing$mpg
pred_RF
Testing
confusionMatrix(Testing,pred_RF)
pred_RF<-predict(fit_RF, newdata = Testing)
confusionMatrix(Testing,pred_RF)
confusionMatrix(Testing$mpg,pred_RF)
pred_RF
class(pred_RF)
pred_RF[1]
rm(list=ls())
data("mtcars")
head(mtcars)
fit<-lm(mpg ~ wt, data = mtcars)
summary(fit1)
summary(fit)
plot(fit)
plot(mtcars$wt, mtcars$mpg)
plot(fit)
plot(mtcars$wt, mtcars$mpg)
abline(fit)
abline(fit, col='red')
summary(firt)
summary(fit)
?data
data()
data(Eu])
data("EuStockMarkets")
head(EuStockMarkets)
?EuStockMarkets
data("euro")
head(euro)
EuStockMarkets$DAX
head(EuStockMarkets[,1])
quakes
head(rock)
?rock
head(diamonds)
?diamonds
data(longley)
head(longley)
?longley
dim(longley)
head(GermanCredit)
data("GermanCredit")
head(GermanCredit)
?GermanCredit
colnames(GermanCredit)
head(Sacramento)
data("Sacramento")
head(Sacramento)
unique(Sacramento$city)
Sacramento$city<-as.factor(Sacramento$city)
Sacramento$type<-as.factor(Sacramento$type)
inTrain<-createDataPartition(y=Sacramento$price, p=0.75, list = 0)
TrainingSet<-Sacramento[inTrain,]
TestingSet<-Sacramento[-inTrain]
TestingSet<-Sacramento[-inTrain,]
trcontrol=trainControl(method="cv", number=5)
fit<-train(price ~ ., data = TrainingSet, method="rf", trControl=trControl)
fit<-train(price ~ ., data = TrainingSet, method="rf", trControl=trcontrol)
pred<-predict(fit,newdata = TestingSet)
confusionMatrix(TestingSet$price,pred)
pred
confusionMatrix(table(TestingSet$price,pred))
confusionMatrix(table(TestingSet$price),table(pred))
head(TestingSet)
TestingSet$price<-NULL
head(TestingSet)
pred<-predict(fit,newdata = TestingSet)
rm(list=ls())
data("Sacramento")
inTrain<-createDataPartition(y=Sacramento$price, p=0.75, list=0)
TrainingSet<-Sacramento[inTrain,]
TestingSet<-Sacramento[-inTrain,]
fit<-train(price ~ ., data = TrainingSet, method="rf")
pred<-predict(fit, newdata = TestingSet)
confusionMatrix(TestingSet$price,pred)
pred
TestingSet$price
TestingSet$price[1]
pred[1]
pred[pred==69307]
rm(list=ls())
library(randomForest)
data("Sacramento")
inTrain<-createDataPartition(y=Sacramento$price)
Training<-Sacramento[inTrain,]
inTrain<-createDataPartition(y=Sacramento$price, p=0.75, list=0)
Training<-Sacramento[inTrain,]
Testing<-Sacramento[-inTrain,]
trcontrol=trainControl(method = "cv",number = 5)
fit<-train(price ~ ., data = Training, method="rf", trControl=trcontrol )
pred<-predict(fit, newdata = Testing)
Testong
Testing
Testing$price
pred
pred[1]
pred[[1]]
unname(pred)
confusionMatrix(Testing$price,unname(pred))
identical(Testing$price,unname(pred))
pred[[1]]
Testing$price[1]
Testing$price[2]
pred[[3]]
sum(Testing$price)
sum(pred)
sum(pred)/sum(Testing$price)
rm(list=ls())
library(tidyverse)
require(tidyverse)
install.packages(tidyverse)
install.packages("tidyverse")
install.packages("reshape2")
install.packages("reshape2")
housing<-read.csv("Documents/R/Housing Tutorial/housing.csv")
housing<-read.csv("Documents/R/Housing Tutorial/housing.csv")
head(hosuing)
head(housing)
housing<-read.csv("Documents/R/Housing Tutorial/housing")
housing<-read.csv("Documents/R/Housing Tutorial/housing.txt")
housing<-read.csv("Documents/R/Housing Tutorial/housing.csv")
head(housing)
housing<-read.csv("Documents/R/Housing Tutorial/housing.csv", sep=",")
rm(list="housing")
housing<-read.csv("Documents/R/Housing Tutorial/housing.csv", sep=",")
head(housing)
summary(housing)
head(housing$total_rooms)
par(mfrow=c(2,5))
colnames(housing)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
library(ggplot2)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
library(reshape2)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)
housing$mean_bedrooms = housing$total_bedrooms/housing$households
housing$mean_rooms = housing$total_rooms/housing$households
drops = c('total_bedrooms', 'total_rooms')
housing = housing[ , !(names(housing) %in% drops)]
heading(housing)
headin(housing)
head(housing)
categories = unique(housing$ocean_proximity)
categories
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)
for(cat in categories){
cat_housing[,cat] = rep(0, times= nrow(cat_housing))
}
cat
head(cat_housing)
for(i in 1:length(cat_housing$ocean_proximity)){
cat = as.character(cat_housing$ocean_proximity[i])
cat_housing[,cat][i] = 1
}
head(cat_housing)
tail(cat_housing)
cat_columns = names(cat_housing)
keep_columns = cat_columns[cat_columns != 'ocean_proximity']
cat_housing = select(cat_housing,one_of(keep_columns))
library(dplyr)
cat_housing = select(cat_housing,one_of(keep_columns))
tail(cat_housing)
colnames(housing)
drops = c('ocean_proximity','median_house_value')
housing_num =  housing[ , !(names(housing) %in% drops)]
head(housing_num)
scaled_housing_num = scale(housing_num)
?scale
cleaned_housing = cbind(cat_housing, scaled_housing_num, median_house_value=housing$median_house_value)
head(cleaned_housing)
write.csv(cleaned_housing,file="Documents/R/Housing Tutorial/cleaned_housing.csv")
set.seed(1738)
sample = sample.int(n = nrow(cleaned_housing), size = floor(.8*nrow(cleaned_housing)), replace = F)
train = cleaned_housing[sample, ] #just the samples
test  = cleaned_housing[-sample, ] #everything but the samples
nrow(train) + nrow(test) == nrow(cleaned_housing)
install.packages("boot")
library(boot)
?cv.glm()
glm_house = glm(median_house_value~median_income+mean_rooms+population, data=cleaned_housing)
k_fold_cv_error = cv.glm(cleaned_housing , glm_house, K=5)
k_fold_cv_error$delta
glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]
glm_cv_rmse
names(glm_house)
glm_house$coefficients
library('randomForest')
train_y = train[,'median_house_value']
train_x = train[, names(train) !='median_house_value']
rf_model = randomForest(train_x, y = train_y , ntree = 500, importance = TRUE)
cleaned_housing
head(cleaned_housing)
rm(list=ls())
mtcars
data(mtcars)
fit<-lm(mpg ~ wt, data=mtcars)
summary(fit)
summary(fit)$coeff
x<-summary(fit)$coeff
x
class(x)
predict(fit, newdata = 123)
runif(100, min= 1, max=10)
x<-runif(100, min= 1, max=10)
predict(fit, newdata = x)
predict(fit, newdata = x)
x<-runif(100, min= 1, max=10)
x
x<-runif(100, min= 1, max=100)
x
head(mtcars)
plot(mtcars$wt, mtcars$wt)
plot(mtcars$wt, mtcars$mpg)
plot(mtcars$wt, mtcars$mpg)
plot(mtcars$wt, mtcars$mpg)
abline(fit)
abline(fit, col='red')
twitter<-readLines(con = "Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt")
news<-readLines(con = "Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.news.txt")
library(tm)
N<-Corpus(news)
N<-Corpus(VectorSource(news))
dtm<-DocumentTermMatrix(N)
inspect(dtm)
rowSums(as.matrix(dtm))
rm(list=ls())
twitter<-readLines("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.blogs.txt")
head(twitter)
length(twitter)
sumOfWords<-sum(sapply(strsplit(twitter," "),length))
sumOfWords
head(unlist(twitter))
class(twitter)
class(unlist(twitter))
?file.size()
?file.size(twitter)
file.size(twitter)
file.info(twitter)
file.info("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.news.txt")
a<-file.info("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.news.txt")
a$size
a$size/1024^2
news<-readLines("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.news.txt")
twitter<-readLines (con="GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt")
twitter<-readLines ("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt")
twitter<-Corpus(VectorSource(twitter))
library(tm)
twitter<-Corpus(VectorSource(twitter))
twtitter<-tm_map(twitter, content_transformer(tolower))
rm(list="twittter")
rm(list="twtitter")
twitter<-tm_map(twitter, content_transformer(tolower))
words(twitter)
sapply(strsplit(twitter, " "), length)
twitter<-readLines ("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt")
sapply(strsplit(twitter, " "), length)
twitter[1]
file.info(twitter)$size
a<-file.info(twitter)
a$size
file.size(twitter)
file.size("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt")
file.size("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt")/1024^2
a<-"hello"
a
a<-"Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt"
file.size(a)
rm(list=ls())
twitterPath<-"Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt"
newsPath<-"Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.news.txt"
blogsPath<"Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.blogs.txt"
blogsPath<"Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.blogs.txt"
twitterPath<-"Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.twitter.txt"
newsPath<-"Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.news.txt"
blogsPath<-"Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/final/en_US/en_US.blogs.txt"
twitter<-readLines(twitterPath)
news<-readLines(newsPath)
blogs<-readLines(blogsPath)
twitterLines<-length(twitter)
newsLines<-length(news)
blogsLine<-length(blogs)
twitterLines
blogsLines
blogsLines<-length(blogs)
blogsLines
newsLines
twitterWords<-sum(sapply(strsplit(twitter,spit=" "),length))
twitterWords<-sum(sapply(strsplit(twitter,split=" "),length))
twitterWords
newsWords<-sum(sapply(strsplit(news,split=" "),length))
newsWords
blogsWords<-sum(sapply(strsplit(blogs,split=" "),length))
blogsWords
twitterWords/twitterLines
newsWords/newsLines
blogsWords/blogsLines
x<-sapply(strsplit(twitter,split=" "),length)
head(x)
mean(x)
rm(list=c("x","blogsLine"))
x<-date.frame()
x<-data.frame()
data.frame(twitter=rbind(twitterWords,twitterLines,),news=rbind(newsWords))
data.frame(twitter=rbind(twitterWords,twitterLines),news=rbind(newsWords,newsLines))
data.frame(twitter=rbind(twitterWords,twitterLines),news=rbind(newsWords,newsLines), row.names = rbind("Words","Lines"))
twitterSize<-file.size(twitterPath)/1024^2
twitterSize
newsSize<-file.size(newsPath)/1024^2
blogsSize<-file.size(blogsPath)/1024^2
newsSize
data.frame(twitter=rbind(twitterWords,twitterLines,twitterSize),news=rbind(newsWords,newsLines), row.names = rbind("Words","Lines"))
data.frame(twitter=rbind(twitterWords,twitterLines,twitterSize),news=rbind(newsWords,newsLines,newsSize),blogs=rbind(blogsWords,blogsLines,blogsSize), row.names = rbind("Words","Lines", "Size (mb)"))
data.frame(twitter=rbind(twitterWords,twitterLines,twitterWords/twitterLines,twitterSize),news=rbind(newsWords,newsLines,newsWords/newsLines,newsSize),blogs=rbind(blogsWords,blogsLines,blogsWords/blogsLines,blogsSize), row.names = rbind("Words","Lines", "Words/Line","Size (mb)"))
library(dplyr)
tbl_df(data.frame(twitter=rbind(twitterWords,twitterLines,twitterWords/twitterLines,twitterSize),news=rbind(newsWords,newsLines,newsWords/newsLines,newsSize),blogs=rbind(blogsWords,blogsLines,blogsWords/blogsLines,blogsSize), row.names = rbind("Words","Lines", "Words/Line","Size (mb)")))
x<-tbl_df(data.frame(twitter=rbind(twitterWords,twitterLines,twitterWords/twitterLines,twitterSize),news=rbind(newsWords,newsLines,newsWords/newsLines,newsSize),blogs=rbind(blogsWords,blogsLines,blogsWords/blogsLines,blogsSize), row.names = rbind("Words","Lines", "Words/Line","Size (mb)")))
x
rm(list='x')
allText<-c(twitter,news,blogs)
allText<-Corpus(VectorSource(allText))
allText<-tm_map(allText, content_transformer(tolower))
allText<-tm_map(allText, removePunctuation)
allText<-tm_map(allText, removeNumbers)
x<-allText
removeNonLetters<-function(x){gsub(pattern = [^a-zA-Z],"",x)}
removeNonLetters<-function(x){gsub(pattern = "^a-zA-Z","",x)}
x<-tm_map(x,removeNonLetters)
identical(x,allText)
rm(list="x"
)
install.packages("RWeka")
library(RWeka)
x<-ngrams(allText,n=1)
rm(list=c("twitter", "blogs", "news"))
head(x)
library(ngram)
get.phrasetable(x)
rm(list=ls())
setwd("Documents/GitHub/DataScience_JH_Coursera_Assignments/10.Capstone/")
twitterPath<-"final/en_US/en_US.twitter.txt"
newsPath<-"final/en_US/en_US.news.txt"
blogsPath<-"final/en_US/en_US.blogs.txt"
twitter<-readLines(twitterPath)
news<-readLines(newsPath)
blogs<-readLines(blogsPath)
allText<-c(twitter,news,blogs)
allText<-sample(allText, 0.1*length(allText))
allText<-paste(allText, collapse = " ")
trigram<-get.phrasetable(ngram(allText, n=3))
library(ngram)
trigram<-get.phrasetable(ngram(allText, n=3))
library(ggplot2)
barplot(head(trigram$freq,10), names.arg = head(trigram$ngrams,10))
ggplot(trigram, aes(x=freq)) + geom_histogram()
ggplot(head(trigram,10), aes(x=freq)) + geom_histogram()
head(trigram)
tail(trigram)
par(mfrow = c(1, 1))
par(mar=c(5,4,2,0))
barplot(trigram[1:25,2],
names.arg=trigram[1:25,1],
col = "blue",
main="Trigrams (Top 25)",
las=2,
ylab = "Frequency")
barplot(head(trigram$freq,15),names.arg = head(trigram$ngrams,15))
barplot(head(trigram$freq,15),names.arg = head(trigram$ngrams,15),las=2)
barplot(head(trigram$freq,15),names.arg = head(trigram$ngrams,15),las=2, col="blue")
par(mar=c(5,4,2,0))
barplot(head(trigram$freq,15),names.arg = head(trigram$ngrams,15),las=2, col="blue")
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue")
par(mfrow = c(1, 1))
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue")
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue", ylim = 4000)
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue", ylim = c(4000,4000)
)
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue")
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue",ylim=range(pretty(c(0, dat))))
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue",ylim=range(pretty(c(0, head(trigram$freq,20)))))
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue",ylim=range(pretty(c(0, head(trigram$freq,20)))), main="Trigram")
barplot(head(trigram$freq,20),names.arg = head(trigram$ngrams,20),las=2, col="blue",ylim=range(pretty(c(0, head(trigram$freq,20)))), main="Trigram", ylab = "frequency")
library(tm)
?PlainTextDocument
library(knitr)
?kable()
?round
x2 <- pi * 100^(-1:3)
x2
round(x2, 3)
round(x2, 0)
?kable
?format
kable(data.frame(twitter=rbind(twitterWords,twitterLines,twitterWords/twitterLines,twitterSize),
news=rbind(newsWords,newsLines,newsWords/newsLines,newsSize),
blogs=rbind(blogsWords,blogsLines,blogsWords/blogsLines,blogsSize),
row.names = rbind("Words","Lines", "Words/Line","Size (mb)")))
rm(list=ls())
library(ngram)
library(tm)
?ngram
?dfm
?tm
??token_ngram
??tokens_ngrams
install.packages(tokens_ngrams)
install.packages("quanteda")
library(quentdata)
library(quanteda)
tokens_ngrams()
?tokens_ngrams()
#Count number of lines in each file
twitterLines<-length(twitter)
newsLines<-length(news)
blogsLines<-length(blogs)
twitterPath<-"final/en_US/en_US.twitter.txt"
newsPath<-"final/en_US/en_US.news.txt"
blogsPath<-"final/en_US/en_US.blogs.txt"
twitter<-readLines(twitterPath)
news<-readLines(newsPath)
blogs<-readLines(blogsPath)
allText<-c(twitter,news,blogs)
allText<-sample(allText, 0.1*length(allText))
bi_gram<-tokens_ngrams(allText, n=2)
?tokens
tokenizedText<-tokens(x=tolower(allText),
remove_punct = TRUE,
remove_twitter = TRUE,
remove_numbers = TRUE,
remove_hyphens = TRUE,
remove_symbols = TRUE,
remove_url = TRUE)
)
tokenizedText<-tokens(x=tolower(allText), remove_punct = TRUE, remove_twitter = TRUE, remove_numbers = TRUE,remove_hyphens = TRUE, remove_symbols = TRUE, remove_url = TRUE)
stemed_words <- tokens_wordstem(tokenizedText, language = "english")
bi_gram <- tokens_ngrams(stemed_words, n = 2)
bi_gram
class(bi_gram)
bi_gram[[1]]
tri_gram <- tokens_ngrams(stemed_words, n = 3)
?dfm
uni_DFM <- dfm(stemed_words)
class(uni_DFM)
uni_dfm[[1]]
uni_DFM[[1]]
uni_DFM[1]
bi_DFM <- dfm(bi_gram)
tri_DFM <- dfm(tri_gram)
uni_DFM <- dfm_trim(uni_DFM, 3)
bi_DFM <- dfm_trim(bi_DFM, 3)
tri_DFM <- dfm_trim(tri_DFM, 3)
class(tri_DFM)
sums_U <- colSums(uni_DFM)
head(sum())
head(sums_U
)
sums_B <- colSums(bi_DFM)
sums_T <- colSums(tri_DFM)
uni_words <- data.table(word_1 = names(sums_U), count = sums_U)
uni_words <- data.table::(word_1 = names(sums_U), count = sums_U)
?data.table
uni_words <- data.table(word_1 = names(sums_U), count = sums_U)
uni_words <- data.table(word_1 = names(sums_U), count = sums_U)
data.table()
vignette(package="data.table") t
vignette(package="data.table")
install.packages("data.tables")
install.packages("data.table")
install.packages("data.table")
uni_words <- data.table(word_1 = names(sums_U), count = sums_U)
library(data.table)
